{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7263eb",
   "metadata": {},
   "source": [
    "# Hadoop Docker Stack Troubleshooting Guide\n",
    "\n",
    "This notebook helps troubleshoot Docker Compose deployment issues for the Hadoop Big Data stack. We'll diagnose the missing image errors and provide solutions.\n",
    "\n",
    "## Problem Overview\n",
    "The error `docker.io/apache/hadoop:3.3.4: not found` indicates that the specified Docker image doesn't exist on Docker Hub. We'll fix this by using alternative, working images.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to check Docker image availability\n",
    "- How to fix Docker Compose configuration issues\n",
    "- How to monitor container health and troubleshoot deployment problems\n",
    "- How to use alternative Docker images for Hadoop components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecef4b7",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing the necessary Python libraries for Docker API interaction and system monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0af6798",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocker\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docker'"
     ]
    }
   ],
   "source": [
    "import docker\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Docker client\n",
    "try:\n",
    "    client = docker.from_env()\n",
    "    print(\"‚úÖ Docker client connected successfully\")\n",
    "    print(f\"Docker version: {client.version()['Version']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to connect to Docker: {e}\")\n",
    "    print(\"Make sure Docker Desktop is running\")\n",
    "\n",
    "# Check Docker system info\n",
    "try:\n",
    "    info = client.info()\n",
    "    print(f\"üìä Containers: {info['Containers']} | Images: {info['Images']} | Memory: {info['MemTotal'] / (1024**3):.1f}GB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not get Docker system info: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54c831",
   "metadata": {},
   "source": [
    "## Section 2: Check Docker Image Availability\n",
    "\n",
    "Let's check if the problematic images exist and find alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0352fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image_exists(image_name):\n",
    "    \"\"\"Check if a Docker image exists locally or remotely\"\"\"\n",
    "    try:\n",
    "        # Check locally first\n",
    "        client.images.get(image_name)\n",
    "        return \"local\", \"‚úÖ Available locally\"\n",
    "    except docker.errors.ImageNotFound:\n",
    "        # Check if it exists on Docker Hub\n",
    "        try:\n",
    "            if \":\" in image_name:\n",
    "                repo, tag = image_name.split(\":\", 1)\n",
    "            else:\n",
    "                repo, tag = image_name, \"latest\"\n",
    "            \n",
    "            # Simple Docker Hub API check\n",
    "            url = f\"https://registry.hub.docker.com/v2/repositories/{repo}/tags/{tag}\"\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return \"remote\", \"‚úÖ Available on Docker Hub\"\n",
    "            else:\n",
    "                return \"missing\", \"‚ùå Not found on Docker Hub\"\n",
    "        except Exception as e:\n",
    "            return \"error\", f\"‚ö†Ô∏è Error checking: {e}\"\n",
    "\n",
    "# Check problematic images from your docker-compose\n",
    "problematic_images = [\n",
    "    \"apache/hadoop:3.3.4\",\n",
    "    \"apache/hive:3.1.3\",\n",
    "    \"bitnami/spark:3.4.1\"\n",
    "]\n",
    "\n",
    "print(\"üîç Checking image availability:\\n\")\n",
    "for image in problematic_images:\n",
    "    status, message = check_image_exists(image)\n",
    "    print(f\"{image:<30} | {message}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° SOLUTION: Use alternative working images\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c84334",
   "metadata": {},
   "source": [
    "## Section 3: Analyze Docker Compose Configuration\n",
    "\n",
    "Let's analyze the current docker-compose.yml file and identify issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68713b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and analyze docker-compose.yml\n",
    "compose_file = \"/home/jovyan/work/../docker-compose.yml\"\n",
    "\n",
    "try:\n",
    "    with open(compose_file, 'r') as f:\n",
    "        compose_content = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"üìÑ Docker Compose Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Check version\n",
    "    if 'version' in compose_content:\n",
    "        print(f\"‚ö†Ô∏è  Version field found: {compose_content['version']} (deprecated in newer Docker Compose)\")\n",
    "    \n",
    "    # Check services and their images\n",
    "    services = compose_content.get('services', {})\n",
    "    print(f\"üìä Found {len(services)} services\")\n",
    "    \n",
    "    print(\"\\nüîç Service Images:\")\n",
    "    for service_name, service_config in services.items():\n",
    "        image = service_config.get('image', 'No image specified')\n",
    "        print(f\"  {service_name:<20} ‚Üí {image}\")\n",
    "        \n",
    "        # Check if image exists\n",
    "        if image != 'No image specified':\n",
    "            status, message = check_image_exists(image)\n",
    "            if status == \"missing\":\n",
    "                print(f\"    ‚ùå {message}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Docker compose file not found: {compose_file}\")\n",
    "    print(\"üí° We'll create a working version\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîß FIXES NEEDED:\")\n",
    "print(\"1. Remove version field (deprecated)\")\n",
    "print(\"2. Replace non-existent images with working alternatives\")\n",
    "print(\"3. Update configuration for compatibility\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc42f911",
   "metadata": {},
   "source": [
    "## Section 4: Create Working Docker Configuration\n",
    "\n",
    "Based on the analysis, we'll create a working configuration using reliable images from the bde2020 project, which provides well-maintained Hadoop ecosystem containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e99497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working Docker Compose configuration\n",
    "working_compose = \"\"\"\n",
    "services:\n",
    "  namenode:\n",
    "    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8\n",
    "    container_name: namenode\n",
    "    restart: always\n",
    "    ports:\n",
    "      - 9870:9870\n",
    "      - 9000:9000\n",
    "    volumes:\n",
    "      - hadoop_namenode:/hadoop/dfs/name\n",
    "    environment:\n",
    "      - CLUSTER_NAME=test\n",
    "    env_file:\n",
    "      - ./hadoop.env\n",
    "\n",
    "  datanode:\n",
    "    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8\n",
    "    container_name: datanode\n",
    "    restart: always\n",
    "    ports:\n",
    "      - 9864:9864\n",
    "    volumes:\n",
    "      - hadoop_datanode:/hadoop/dfs/data\n",
    "    environment:\n",
    "      SERVICE_PRECONDITION: \"namenode:9870\"\n",
    "    env_file:\n",
    "      - ./hadoop.env\n",
    "\n",
    "  resourcemanager:\n",
    "    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8\n",
    "    container_name: resourcemanager\n",
    "    restart: always\n",
    "    ports:\n",
    "      - 8088:8088\n",
    "    environment:\n",
    "      SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864\"\n",
    "    env_file:\n",
    "      - ./hadoop.env\n",
    "\n",
    "  nodemanager1:\n",
    "    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8\n",
    "    container_name: nodemanager\n",
    "    restart: always\n",
    "    ports:\n",
    "      - 8042:8042\n",
    "    environment:\n",
    "      SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\"\n",
    "    env_file:\n",
    "      - ./hadoop.env\n",
    "\n",
    "  historyserver:\n",
    "    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8\n",
    "    container_name: historyserver\n",
    "    restart: always\n",
    "    ports:\n",
    "      - 8188:8188\n",
    "    environment:\n",
    "      SERVICE_PRECONDITION: \"namenode:9000 namenode:9870 datanode:9864 resourcemanager:8088\"\n",
    "    volumes:\n",
    "      - hadoop_historyserver:/hadoop/yarn/timeline\n",
    "    env_file:\n",
    "      - ./hadoop.env\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    container_name: postgres-hive\n",
    "    restart: always\n",
    "    environment:\n",
    "      POSTGRES_DB: metastore\n",
    "      POSTGRES_USER: hive\n",
    "      POSTGRES_PASSWORD: hive123\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "  hive-metastore:\n",
    "    image: apache/hive:4.0.0\n",
    "    container_name: hive-metastore\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"9083:9083\"\n",
    "    environment:\n",
    "      SERVICE_PRECONDITION: \"namenode:9870 datanode:9864 postgres:5432\"\n",
    "      DB_DRIVER: postgres\n",
    "      SERVICE_NAME: 'metastore'\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    volumes:\n",
    "      - ./init/init-hive-db.sql:/docker-entrypoint-initdb.d/init-hive-db.sql\n",
    "\n",
    "  spark-master:\n",
    "    image: bitnami/spark:3.3\n",
    "    container_name: spark-master\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "      - \"7077:7077\"\n",
    "    environment:\n",
    "      - SPARK_MODE=master\n",
    "      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n",
    "      - SPARK_RPC_ENCRYPTION_ENABLED=no\n",
    "      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n",
    "      - SPARK_SSL_ENABLED=no\n",
    "\n",
    "  spark-worker:\n",
    "    image: bitnami/spark:3.3\n",
    "    container_name: spark-worker\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      - SPARK_MODE=worker\n",
    "      - SPARK_MASTER_URL=spark://spark-master:7077\n",
    "      - SPARK_WORKER_MEMORY=1G\n",
    "      - SPARK_WORKER_CORES=1\n",
    "      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n",
    "      - SPARK_RPC_ENCRYPTION_ENABLED=no\n",
    "      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n",
    "      - SPARK_SSL_ENABLED=no\n",
    "    depends_on:\n",
    "      - spark-master\n",
    "\n",
    "volumes:\n",
    "  hadoop_namenode:\n",
    "  hadoop_datanode:\n",
    "  hadoop_historyserver:\n",
    "  postgres_data:\n",
    "\"\"\"\n",
    "\n",
    "# Write the working configuration\n",
    "output_file = \"/home/jovyan/work/docker-compose-working.yml\"\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(working_compose)\n",
    "\n",
    "print(\"‚úÖ Created working Docker Compose configuration!\")\n",
    "print(f\"üìÅ Saved to: {output_file}\")\n",
    "print(\"\\nüîß Key improvements:\")\n",
    "print(\"‚Ä¢ Uses verified bde2020 Hadoop images\")\n",
    "print(\"‚Ä¢ Uses official PostgreSQL for Hive Metastore\")\n",
    "print(\"‚Ä¢ Uses Bitnami Spark images (well-maintained)\")\n",
    "print(\"‚Ä¢ Removed deprecated version field\")\n",
    "print(\"‚Ä¢ Added proper service dependencies\")\n",
    "print(\"‚Ä¢ Configured persistent volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ecc79",
   "metadata": {},
   "source": [
    "## Section 5: Environment Configuration\n",
    "\n",
    "We need a proper environment file for Hadoop configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69196acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hadoop environment configuration\n",
    "hadoop_env = \"\"\"\n",
    "CORE_CONF_fs_defaultFS=hdfs://namenode:9000\n",
    "CORE_CONF_hadoop_http_staticuser_user=root\n",
    "CORE_CONF_hadoop_proxyuser_hue_hosts=*\n",
    "CORE_CONF_hadoop_proxyuser_hue_groups=*\n",
    "CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec\n",
    "\n",
    "HDFS_CONF_dfs_webhdfs_enabled=true\n",
    "HDFS_CONF_dfs_permissions_enabled=false\n",
    "HDFS_CONF_dfs_nameservices=cluster1\n",
    "HDFS_CONF_dfs_ha_namenodes_cluster1=nn1,nn2\n",
    "HDFS_CONF_dfs_namenode_rpc_address_cluster1_nn1=namenode:9000\n",
    "HDFS_CONF_dfs_namenode_http_address_cluster1_nn1=namenode:9870\n",
    "HDFS_CONF_dfs_replication=1\n",
    "\n",
    "YARN_CONF_yarn_log___aggregation___enable=true\n",
    "YARN_CONF_yarn_log_server_url=http://historyserver:8188/applicationhistory/logs/\n",
    "YARN_CONF_yarn_resourcemanager_recovery_enabled=true\n",
    "YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore\n",
    "YARN_CONF_yarn_resourcemanager_hostname=resourcemanager\n",
    "YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032\n",
    "YARN_CONF_yarn_resourcemanager_scheduler_address=resourcemanager:8030\n",
    "YARN_CONF_yarn_resourcemanager_resource___tracker_address=resourcemanager:8031\n",
    "YARN_CONF_yarn_timeline___service_enabled=true\n",
    "YARN_CONF_yarn_timeline___service_generic___application___history_enabled=true\n",
    "YARN_CONF_yarn_timeline___service_hostname=historyserver\n",
    "YARN_CONF_mapreduce_map_output_compress=true\n",
    "YARN_CONF_mapred_map_output_compress_codec=org.apache.hadoop.io.compress.SnappyCodec\n",
    "YARN_CONF_yarn_nodemanager_resource_memory___mb=1400\n",
    "YARN_CONF_yarn_scheduler_maximum___allocation___mb=1400\n",
    "YARN_CONF_yarn_scheduler_minimum___allocation___mb=128\n",
    "YARN_CONF_yarn_nodemanager_vmem___check___enabled=false\n",
    "\"\"\"\n",
    "\n",
    "# Write the environment file\n",
    "env_file = \"/home/jovyan/work/hadoop.env\"\n",
    "with open(env_file, 'w') as f:\n",
    "    f.write(hadoop_env)\n",
    "\n",
    "print(\"‚úÖ Created Hadoop environment configuration!\")\n",
    "print(f\"üìÅ Saved to: {env_file}\")\n",
    "print(\"\\nüîß Key configurations:\")\n",
    "print(\"‚Ä¢ HDFS default filesystem: hdfs://namenode:9000\")\n",
    "print(\"‚Ä¢ Web UI enabled on port 9870\")\n",
    "print(\"‚Ä¢ YARN ResourceManager on port 8032\")\n",
    "print(\"‚Ä¢ Memory limits set for single-node setup\")\n",
    "print(\"‚Ä¢ Replication factor set to 1 (for testing)\")\n",
    "print(\"‚Ä¢ Timeline service enabled for job history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992cf89",
   "metadata": {},
   "source": [
    "## Section 6: Deploy and Monitor\n",
    "\n",
    "Now let's start the working environment and monitor the services as they come online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef2e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to monitor Docker containers\n",
    "def monitor_containers():\n",
    "    \"\"\"Monitor the status of all containers in our stack\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['docker', 'ps', '--format', 'table {{.Names}}\\t{{.Status}}\\t{{.Ports}}'], \n",
    "                              capture_output=True, text=True, check=True)\n",
    "        print(\"üê≥ CONTAINER STATUS:\")\n",
    "        print(\"=\"*80)\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Check if all expected containers are running\n",
    "        expected_containers = [\n",
    "            'namenode', 'datanode', 'resourcemanager', 'nodemanager', \n",
    "            'historyserver', 'postgres-hive', 'hive-metastore', \n",
    "            'spark-master', 'spark-worker'\n",
    "        ]\n",
    "        \n",
    "        running_containers = []\n",
    "        for line in result.stdout.split('\\n')[1:]:  # Skip header\n",
    "            if line.strip():\n",
    "                container_name = line.split('\\t')[0]\n",
    "                running_containers.append(container_name)\n",
    "        \n",
    "        print(\"\\nüìä SERVICE SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        for container in expected_containers:\n",
    "            status = \"‚úÖ Running\" if container in running_containers else \"‚ùå Not Running\"\n",
    "            print(f\"{container:<20} {status}\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error checking containers: {e}\")\n",
    "        return False\n",
    "    \n",
    "    return len(running_containers) > 0\n",
    "\n",
    "# Function to check service health\n",
    "def check_service_health():\n",
    "    \"\"\"Check if web UIs are accessible\"\"\"\n",
    "    services = {\n",
    "        'Hadoop NameNode': 'http://localhost:9870',\n",
    "        'YARN ResourceManager': 'http://localhost:8088', \n",
    "        'Spark Master': 'http://localhost:8080',\n",
    "        'Spark Worker': 'http://localhost:8081'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüåê WEB UI HEALTH CHECK:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for service_name, url in services.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"‚úÖ {service_name:<25} ‚Üí {url}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  {service_name:<25} ‚Üí {url} (Status: {response.status_code})\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            print(f\"‚ùå {service_name:<25} ‚Üí {url} (Not accessible)\")\n",
    "\n",
    "# Start monitoring\n",
    "print(\"üöÄ STARTING DOCKER ENVIRONMENT MONITORING\")\n",
    "print(\"=\"*60)\n",
    "monitor_containers()\n",
    "check_service_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4cabe",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "**To use this troubleshooting notebook:**\n",
    "\n",
    "1. **Copy the working files to your Docker directory:**\n",
    "   ```bash\n",
    "   cp /home/jovyan/work/docker-compose-working.yml /path/to/your/docker/directory/docker-compose.yml\n",
    "   cp /home/jovyan/work/hadoop.env /path/to/your/docker/directory/hadoop.env\n",
    "   ```\n",
    "\n",
    "2. **Start the environment:**\n",
    "   ```bash\n",
    "   cd /path/to/your/docker/directory\n",
    "   docker-compose up -d\n",
    "   ```\n",
    "\n",
    "3. **Monitor progress:** Run the monitoring cell above to check container status\n",
    "\n",
    "4. **Access web interfaces:**\n",
    "   - Hadoop NameNode: http://localhost:9870\n",
    "   - YARN ResourceManager: http://localhost:8088\n",
    "   - Spark Master: http://localhost:8080\n",
    "   - Spark Worker: http://localhost:8081\n",
    "\n",
    "**Troubleshooting tips:**\n",
    "- If containers fail to start, check `docker-compose logs [service-name]`\n",
    "- Ensure ports 9870, 9000, 8088, 8080, 8081, 5432 are not in use\n",
    "- Wait 2-3 minutes for all services to fully initialize\n",
    "- PostgreSQL and Hadoop services have startup dependencies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
