services:
  # NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-namenode
    hostname: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - CORE_CONF_hadoop_http_staticuser_user=root
      - HDFS_CONF_dfs_namenode_name_dir=/hadoop/dfs/name
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_replication=1
      - HDFS_CONF_dfs_permissions_enabled=false
      - HDFS_CONF_dfs_webhdfs_enabled=true
    volumes:
      - ./volumes/namenode:/hadoop/dfs/name
      - ./volumes/shared:/shared
    command: ["hdfs", "namenode"]
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 3

  # DataNode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-datanode
    hostname: datanode
    ports:
      - "9864:9864"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_data_dir=/hadoop/dfs/data
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:9864
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - ./volumes/datanode:/hadoop/dfs/data
      - ./volumes/shared:/shared
    command: ["hdfs", "datanode"]
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata-network

  # ResourceManager
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-resourcemanager
    hostname: resourcemanager
    ports:
      - "8088:8088"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032
      - YARN_CONF_yarn_resourcemanager_webapp_address=0.0.0.0:8088
      - YARN_CONF_yarn_nodemanager_aux_services=mapreduce_shuffle
      - YARN_CONF_yarn_nodemanager_resource_memory_mb=2048
      - YARN_CONF_yarn_nodemanager_resource_cpu_vcores=2
    command: ["yarn", "resourcemanager"]
    depends_on:
      namenode:
        condition: service_healthy
    networks:
      - bigdata-network

  # NodeManager
  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: hadoop-nodemanager
    hostname: nodemanager
    ports:
      - "8042:8042"
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
      - YARN_CONF_yarn_nodemanager_aux_services=mapreduce_shuffle
      - YARN_CONF_yarn_nodemanager_resource_memory_mb=2048
      - YARN_CONF_yarn_nodemanager_resource_cpu_vcores=2
      - YARN_CONF_yarn_nodemanager_webapp_address=0.0.0.0:8042
    command: ["yarn", "nodemanager"]
    depends_on:
      - resourcemanager
    networks:
      - bigdata-network

  # MySQL for Hive Metastore
  mysql:
    image: mysql:8.0
    container_name: hive-mysql
    hostname: mysql
    ports:
      - "3306:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=rootpassword
      - MYSQL_DATABASE=hive_metastore
      - MYSQL_USER=hive
      - MYSQL_PASSWORD=hivepassword
    volumes:
      - ./volumes/mysql:/var/lib/mysql
      - ./init/init-hive-db.sql:/docker-entrypoint-initdb.d/init-hive-db.sql
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Hive Metastore
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    hostname: hive-metastore
    ports:
      - "9083:9083"
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=mysql
      - SERVICE_OPTS=-Djavax.jdo.option.ConnectionDriverName=com.mysql.cj.jdbc.Driver -Djavax.jdo.option.ConnectionURL=jdbc:mysql://mysql:3306/hive_metastore?useSSL=false -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hivepassword
    depends_on:
      mysql:
        condition: service_healthy
      namenode:
        condition: service_healthy
    networks:
      - bigdata-network

  # HiveServer2
  hiveserver2:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server2
    hostname: hiveserver2
    ports:
      - "10000:10000"
      - "10002:10002"
    environment:
      - SERVICE_NAME=hiveserver2
      - SERVICE_OPTS=-Dhive.metastore.uris=thrift://hive-metastore:9083
      - IS_RESUME=true
    depends_on:
      - hive-metastore
    networks:
      - bigdata-network

  # Spark Master
  spark-master:
    image: bitnami/spark:3.3
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "18080:18080"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8080
    volumes:
      - ./volumes/spark-master:/opt/bitnami/spark/work
      - ./volumes/shared:/shared
    networks:
      - bigdata-network

  # Spark Worker
  spark-worker:
    image: bitnami/spark:3.3
    container_name: spark-worker
    hostname: spark-worker
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_WORKER_WEBUI_PORT=8081
    depends_on:
      - spark-master
    volumes:
      - ./volumes/spark-worker:/opt/bitnami/spark/work
      - ./volumes/shared:/shared
    networks:
      - bigdata-network

  # Jupyter Notebook with PySpark
  jupyter:
    image: jupyter/pyspark-notebook:latest
    container_name: jupyter-pyspark
    hostname: jupyter
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - SPARK_MASTER=spark://spark-master:7077
      - PYSPARK_SUBMIT_ARGS=--master spark://spark-master:7077 pyspark-shell
    volumes:
      - ./volumes/notebooks:/home/jovyan/work
      - ./volumes/shared:/shared
    depends_on:
      - spark-master
    networks:
      - bigdata-network
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''

networks:
  bigdata-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  namenode:
  datanode:
  mysql:
  spark-master:
  spark-worker:
  notebooks:
  shared: